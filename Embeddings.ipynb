{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch_geometric.nn.models import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load data splits\n",
    "# dataset = 'fer2013'\n",
    "dataset = 'ck'\n",
    "train_data_path = dataset + '_data/train_data_70_20_10.pkl'\n",
    "val_data_path = dataset + '_data/val_data_70_20_10.pkl'\n",
    "test_data_path = dataset + '_data/test_data_70_20_10.pkl'\n",
    "\n",
    "with open(train_data_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(val_data_path, 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "with open(test_data_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "adjacency_matrix = np.loadtxt('standard_mesh_adj_matrix.csv', delimiter=',')\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "# Add batch attribute to each data object\n",
    "for data in train_data:\n",
    "    data.batch = torch.zeros(data.x.size(0), dtype=torch.long)\n",
    "for data in val_data:\n",
    "    data.batch = torch.zeros(data.x.size(0), dtype=torch.long)\n",
    "for data in test_data:\n",
    "    data.batch = torch.zeros(data.x.size(0), dtype=torch.long)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "291"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from karateclub import Graph2Vec\n",
    "\n",
    "train_graphs = [to_networkx(data) for data in train_data]\n",
    "val_graphs = [to_networkx(data) for data in val_data]\n",
    "test_graphs = [to_networkx(data) for data in test_data]\n",
    "all_graphs = train_graphs + val_graphs + test_graphs\n",
    "\n",
    "graph2vec = Graph2Vec()\n",
    "graph2vec.fit(all_graphs)\n",
    "\n",
    "all_embeddings = torch.tensor(graph2vec.get_embedding(), dtype=torch.float32)\n",
    "\n",
    "train_data_embeddings = all_embeddings[:len(train_graphs)]\n",
    "val_data_embeddings = all_embeddings[len(train_graphs):len(train_graphs) + len(val_graphs)]\n",
    "test_data_embeddings = all_embeddings[len(train_graphs) + len(val_graphs):]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468]),\n Data(x=[468, 3], edge_index=[2, 2644], y=[1], bbox=[6], batch=[468])]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_labels = torch.tensor(([data.y for data in train_data]))\n",
    "val_labels = torch.tensor(([data.y for data in val_data]))\n",
    "test_labels = torch.tensor(([data.y for data in test_data]))\n",
    "\n",
    "# Update TensorDatasets with labels\n",
    "train_dataset = TensorDataset(train_data_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_data_embeddings, val_labels)\n",
    "test_dataset = TensorDataset(test_data_embeddings, test_labels)\n",
    "\n",
    "# Update DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.1404, 0.1256, 0.1549,  ..., 0.1707, 0.0206, 0.0878],\n",
      "        [0.1371, 0.1180, 0.1566,  ..., 0.1710, 0.0230, 0.0805],\n",
      "        [0.1422, 0.1277, 0.1553,  ..., 0.1867, 0.0141, 0.0822],\n",
      "        ...,\n",
      "        [0.1398, 0.1208, 0.1682,  ..., 0.1810, 0.0216, 0.0781],\n",
      "        [0.1424, 0.1198, 0.1598,  ..., 0.1710, 0.0126, 0.0863],\n",
      "        [0.1394, 0.1271, 0.1591,  ..., 0.1809, 0.0227, 0.0818]]), tensor([0, 5, 5, 0, 2, 5, 0, 1, 1, 3, 0, 3, 1, 0, 5, 4, 5, 7, 6, 1, 3, 0, 4, 1,\n",
      "        4, 1, 5, 3, 1, 0, 0, 0])]\n",
      "[tensor([[0.1373, 0.1114, 0.1579,  ..., 0.1715, 0.0143, 0.0770],\n",
      "        [0.1465, 0.1277, 0.1621,  ..., 0.1723, 0.0234, 0.0849],\n",
      "        [0.1396, 0.1256, 0.1609,  ..., 0.1683, 0.0112, 0.0926],\n",
      "        ...,\n",
      "        [0.1429, 0.1271, 0.1516,  ..., 0.1700, 0.0252, 0.0780],\n",
      "        [0.1447, 0.1195, 0.1602,  ..., 0.1757, 0.0155, 0.0913],\n",
      "        [0.1451, 0.1230, 0.1558,  ..., 0.1740, 0.0161, 0.0822]]), tensor([0, 3, 3, 0, 1, 0, 6, 0, 3, 4, 6, 0, 0, 0, 0, 5, 3, 6, 0, 2, 1, 1, 1, 6,\n",
      "        3, 0, 4, 0, 5, 2, 3, 3])]\n",
      "[tensor([[0.1364, 0.1210, 0.1602,  ..., 0.1716, 0.0194, 0.0791],\n",
      "        [0.1473, 0.1204, 0.1609,  ..., 0.1707, 0.0163, 0.0890],\n",
      "        [0.1442, 0.1250, 0.1699,  ..., 0.1738, 0.0171, 0.0795],\n",
      "        ...,\n",
      "        [0.1459, 0.1281, 0.1625,  ..., 0.1750, 0.0162, 0.0873],\n",
      "        [0.1458, 0.1239, 0.1725,  ..., 0.1823, 0.0226, 0.0909],\n",
      "        [0.1464, 0.1171, 0.1537,  ..., 0.1797, 0.0237, 0.0924]]), tensor([0, 3, 5, 3, 6, 1, 1, 6, 3, 3, 1, 4, 3, 0, 1, 1, 2, 6, 0, 0, 5, 3, 5, 1,\n",
      "        3, 6, 3, 2, 5, 0, 1, 2])]\n",
      "[tensor([[0.1456, 0.1178, 0.1660,  ..., 0.1758, 0.0189, 0.0870],\n",
      "        [0.1483, 0.1217, 0.1584,  ..., 0.1814, 0.0113, 0.0825],\n",
      "        [0.1462, 0.1257, 0.1604,  ..., 0.1761, 0.0129, 0.0959],\n",
      "        ...,\n",
      "        [0.1385, 0.1245, 0.1691,  ..., 0.1679, 0.0112, 0.0859],\n",
      "        [0.1404, 0.1193, 0.1515,  ..., 0.1692, 0.0190, 0.0885],\n",
      "        [0.1481, 0.1228, 0.1644,  ..., 0.1806, 0.0192, 0.0927]]), tensor([3, 3, 6, 1, 5, 0, 1, 3, 3, 1, 5, 3, 2, 0, 1, 5, 0, 3, 3, 1, 3, 2, 1, 5,\n",
      "        6, 2, 0, 2, 0, 4, 3, 0])]\n",
      "[tensor([[0.1397, 0.1195, 0.1600,  ..., 0.1838, 0.0205, 0.0933],\n",
      "        [0.1439, 0.1211, 0.1664,  ..., 0.1745, 0.0093, 0.0832],\n",
      "        [0.1397, 0.1188, 0.1549,  ..., 0.1838, 0.0200, 0.0816],\n",
      "        ...,\n",
      "        [0.1481, 0.1291, 0.1657,  ..., 0.1703, 0.0222, 0.0818],\n",
      "        [0.1508, 0.1291, 0.1583,  ..., 0.1685, 0.0171, 0.0839],\n",
      "        [0.1432, 0.1168, 0.1635,  ..., 0.1748, 0.0136, 0.0881]]), tensor([6, 5, 0, 2, 5, 6, 0, 0, 0, 4, 1, 5, 3, 1, 3, 1, 1, 2, 0, 4, 1, 5, 1, 7,\n",
      "        1, 5, 5, 5, 6, 3, 7, 3])]\n",
      "[tensor([[0.1395, 0.1135, 0.1650,  ..., 0.1777, 0.0102, 0.0766],\n",
      "        [0.1402, 0.1294, 0.1598,  ..., 0.1791, 0.0181, 0.0874],\n",
      "        [0.1427, 0.1304, 0.1551,  ..., 0.1688, 0.0192, 0.0868],\n",
      "        ...,\n",
      "        [0.1415, 0.1243, 0.1546,  ..., 0.1687, 0.0213, 0.0858],\n",
      "        [0.1463, 0.1219, 0.1578,  ..., 0.1723, 0.0181, 0.0878],\n",
      "        [0.1323, 0.1217, 0.1634,  ..., 0.1674, 0.0118, 0.0854]]), tensor([3, 5, 6, 6, 2, 2, 6, 3, 7, 3, 6, 4, 0, 6, 0, 4, 4, 1, 0, 1, 0, 3, 0, 5,\n",
      "        4, 0, 7, 5, 1, 5, 1, 0])]\n",
      "[tensor([[0.1364, 0.1298, 0.1568,  ..., 0.1792, 0.0156, 0.0913],\n",
      "        [0.1452, 0.1197, 0.1623,  ..., 0.1777, 0.0181, 0.0861],\n",
      "        [0.1359, 0.1164, 0.1677,  ..., 0.1766, 0.0128, 0.0914],\n",
      "        ...,\n",
      "        [0.1373, 0.1206, 0.1613,  ..., 0.1774, 0.0148, 0.0911],\n",
      "        [0.1415, 0.1204, 0.1561,  ..., 0.1814, 0.0192, 0.0894],\n",
      "        [0.1427, 0.1280, 0.1606,  ..., 0.1813, 0.0118, 0.0812]]), tensor([2, 3, 0, 3, 3, 0, 0, 5, 4, 7, 0, 0, 0, 1, 3, 1, 5, 3, 3, 6, 7, 5, 2, 1,\n",
      "        3, 1, 0, 0, 3, 1, 7, 0])]\n",
      "[tensor([[0.1443, 0.1230, 0.1597,  ..., 0.1797, 0.0180, 0.0913],\n",
      "        [0.1462, 0.1206, 0.1600,  ..., 0.1754, 0.0168, 0.0934],\n",
      "        [0.1429, 0.1162, 0.1586,  ..., 0.1819, 0.0174, 0.0837],\n",
      "        ...,\n",
      "        [0.1504, 0.1222, 0.1531,  ..., 0.1825, 0.0223, 0.0854],\n",
      "        [0.1466, 0.1149, 0.1571,  ..., 0.1830, 0.0196, 0.0884],\n",
      "        [0.1383, 0.1200, 0.1649,  ..., 0.1809, 0.0149, 0.0841]]), tensor([1, 0, 0, 3, 1, 5, 3, 0, 6, 0, 3, 7, 3, 7, 3, 7, 3, 4, 0, 3, 5, 1, 6, 6,\n",
      "        0, 1, 7, 5, 0, 0, 5, 5])]\n",
      "[tensor([[0.1465, 0.1257, 0.1733,  ..., 0.1768, 0.0171, 0.0826],\n",
      "        [0.1393, 0.1206, 0.1548,  ..., 0.1726, 0.0267, 0.0856],\n",
      "        [0.1391, 0.1137, 0.1561,  ..., 0.1794, 0.0226, 0.0841],\n",
      "        ...,\n",
      "        [0.1478, 0.1149, 0.1679,  ..., 0.1765, 0.0183, 0.0867],\n",
      "        [0.1387, 0.1182, 0.1617,  ..., 0.1743, 0.0180, 0.0860],\n",
      "        [0.1395, 0.1155, 0.1625,  ..., 0.1780, 0.0148, 0.0890]]), tensor([6, 6, 5, 3, 5, 6, 5, 3, 6, 2, 3, 1, 3, 3, 3, 6, 5, 2, 0, 0, 1, 6, 0, 4,\n",
      "        0, 1, 5, 0, 6, 3, 5, 6])]\n",
      "[tensor([[ 1.3798e-01,  1.1704e-01,  1.5510e-01, -9.1002e-02, -1.6592e-01,\n",
      "         -9.3294e-02, -5.0731e-02,  8.8707e-02, -7.8095e-02, -1.0056e-01,\n",
      "         -7.7862e-02,  1.1951e-01, -8.2980e-02,  1.0916e-01,  1.1261e-01,\n",
      "          8.8227e-02,  3.7435e-02,  8.9414e-02, -5.1117e-02,  4.2746e-02,\n",
      "          2.6938e-02, -2.9816e-02, -1.7847e-02, -9.7306e-02, -4.2352e-02,\n",
      "         -5.9166e-02,  1.6204e-01, -6.9545e-02, -1.1442e-01, -4.5117e-02,\n",
      "         -1.3901e-01, -1.2903e-01, -3.2905e-02, -4.5505e-02, -1.2212e-01,\n",
      "          3.9819e-02,  6.7653e-03,  1.3450e-01,  1.5887e-01, -7.9773e-02,\n",
      "         -1.3006e-01,  2.8409e-02, -1.1451e-01,  2.2364e-02,  1.4305e-01,\n",
      "          7.3954e-02, -7.4242e-03, -3.0241e-02,  1.0303e-02, -1.2646e-01,\n",
      "          1.7023e-01,  1.4811e-01,  2.8715e-02,  7.9076e-02,  1.3606e-01,\n",
      "          7.1177e-02,  1.2771e-01, -1.4187e-01, -1.4092e-02,  1.1773e-01,\n",
      "         -1.4622e-03,  1.2137e-01,  8.2008e-02,  4.9757e-02,  5.9576e-02,\n",
      "          1.1407e-01, -2.2929e-02,  8.8516e-02,  9.6935e-02,  4.2564e-02,\n",
      "         -1.6970e-01, -1.5863e-01,  1.0325e-01,  7.0899e-02, -5.7293e-02,\n",
      "         -6.3142e-02, -8.4534e-02, -6.4074e-03, -9.5688e-02, -4.2490e-02,\n",
      "         -9.3819e-02, -1.1643e-01, -1.3617e-01, -4.4688e-02, -9.8116e-02,\n",
      "          1.1650e-01,  4.6929e-02, -1.1089e-01, -4.6308e-02, -7.6487e-02,\n",
      "          4.0629e-02, -1.8234e-02,  2.8424e-02,  1.7548e-01, -2.3199e-02,\n",
      "          1.3189e-01, -6.8150e-02,  3.5468e-02,  2.5033e-02, -6.3410e-02,\n",
      "         -1.5899e-02, -1.3917e-01,  7.9032e-02,  1.2558e-01,  2.1830e-01,\n",
      "          1.1909e-01,  1.3669e-01, -1.4228e-01, -4.4080e-02, -9.2841e-02,\n",
      "         -3.9896e-02, -4.6568e-02, -6.6859e-02, -2.4296e-02,  6.9632e-02,\n",
      "          1.1338e-01, -9.0111e-02,  1.2328e-01,  6.7103e-02, -1.6156e-01,\n",
      "          3.5804e-02,  4.1857e-02,  1.0047e-01,  1.3420e-01, -4.5732e-02,\n",
      "          1.6513e-01,  1.3479e-02,  7.9720e-02],\n",
      "        [ 1.4186e-01,  1.2673e-01,  1.5860e-01, -9.2337e-02, -1.6181e-01,\n",
      "         -9.2956e-02, -4.8119e-02,  1.0616e-01, -8.3541e-02, -8.8035e-02,\n",
      "         -7.7199e-02,  1.3110e-01, -9.1127e-02,  1.1472e-01,  1.0383e-01,\n",
      "          8.7105e-02,  3.9576e-02,  9.1146e-02, -4.6748e-02,  5.2051e-02,\n",
      "          2.3189e-02, -3.7513e-02, -1.3883e-02, -9.6032e-02, -4.1600e-02,\n",
      "         -5.9817e-02,  1.6680e-01, -6.9457e-02, -1.0930e-01, -3.6501e-02,\n",
      "         -1.3872e-01, -1.1721e-01, -3.1266e-02, -3.6980e-02, -1.1373e-01,\n",
      "          3.8700e-02, -3.3137e-03,  1.3653e-01,  1.5941e-01, -7.5144e-02,\n",
      "         -1.3103e-01,  3.3230e-02, -1.2468e-01,  2.0670e-02,  1.4736e-01,\n",
      "          7.5132e-02, -7.5058e-03, -3.4099e-02,  3.2743e-03, -1.2070e-01,\n",
      "          1.7721e-01,  1.5827e-01,  2.7541e-02,  7.6688e-02,  1.2231e-01,\n",
      "          6.7265e-02,  1.3706e-01, -1.3575e-01, -1.9394e-02,  1.1467e-01,\n",
      "         -7.5495e-04,  1.1745e-01,  8.6251e-02,  4.4617e-02,  6.3120e-02,\n",
      "          1.1280e-01, -2.1279e-02,  8.0379e-02,  9.0856e-02,  4.2226e-02,\n",
      "         -1.6681e-01, -1.6184e-01,  1.0192e-01,  7.0984e-02, -5.4519e-02,\n",
      "         -5.9849e-02, -8.5023e-02, -2.9155e-03, -8.5987e-02, -3.2166e-02,\n",
      "         -1.0109e-01, -1.1670e-01, -1.4407e-01, -5.1210e-02, -8.7842e-02,\n",
      "          1.1963e-01,  5.2055e-02, -1.2020e-01, -5.0964e-02, -7.2464e-02,\n",
      "          3.4860e-02, -2.4727e-02,  2.7949e-02,  1.7327e-01, -1.3207e-02,\n",
      "          1.4590e-01, -6.1264e-02,  4.5893e-02,  2.4928e-02, -7.1769e-02,\n",
      "         -2.1497e-02, -1.4403e-01,  8.2873e-02,  1.2320e-01,  2.2242e-01,\n",
      "          1.3094e-01,  1.4786e-01, -1.4430e-01, -4.8633e-02, -9.5061e-02,\n",
      "         -4.5818e-02, -4.0975e-02, -6.5958e-02, -2.8575e-02,  6.4879e-02,\n",
      "          1.0596e-01, -8.8079e-02,  1.2852e-01,  7.4431e-02, -1.5465e-01,\n",
      "          3.9257e-02,  3.6187e-02,  1.0690e-01,  1.3461e-01, -4.2841e-02,\n",
      "          1.8506e-01,  1.1969e-02,  8.2873e-02],\n",
      "        [ 1.4821e-01,  1.1674e-01,  1.6008e-01, -1.0007e-01, -1.5814e-01,\n",
      "         -9.0590e-02, -4.5477e-02,  9.6293e-02, -8.0743e-02, -1.0396e-01,\n",
      "         -7.9368e-02,  1.2158e-01, -8.0673e-02,  1.0977e-01,  1.0433e-01,\n",
      "          7.8726e-02,  5.1628e-02,  8.9479e-02, -5.0129e-02,  4.3262e-02,\n",
      "          2.8109e-02, -3.5152e-02, -2.0801e-02, -1.0312e-01, -4.4533e-02,\n",
      "         -5.7327e-02,  1.5370e-01, -6.4567e-02, -1.0509e-01, -4.2873e-02,\n",
      "         -1.3546e-01, -1.2277e-01, -3.9693e-02, -3.8805e-02, -1.2273e-01,\n",
      "          4.0325e-02,  4.8961e-03,  1.3847e-01,  1.5704e-01, -6.8875e-02,\n",
      "         -1.3122e-01,  3.3949e-02, -1.1756e-01,  2.1952e-02,  1.5634e-01,\n",
      "          6.8075e-02, -8.1599e-03, -1.9573e-02,  7.3653e-04, -1.1651e-01,\n",
      "          1.7965e-01,  1.5492e-01,  2.8794e-02,  7.2317e-02,  1.3211e-01,\n",
      "          6.5484e-02,  1.3023e-01, -1.3941e-01, -1.5641e-02,  1.1331e-01,\n",
      "         -1.3858e-04,  1.1395e-01,  9.0270e-02,  4.8635e-02,  5.7731e-02,\n",
      "          1.0849e-01, -2.3863e-02,  8.4862e-02,  8.6111e-02,  4.3792e-02,\n",
      "         -1.7429e-01, -1.5609e-01,  1.0252e-01,  7.8361e-02, -5.8999e-02,\n",
      "         -6.3039e-02, -8.2740e-02, -6.2230e-03, -8.9508e-02, -4.1090e-02,\n",
      "         -8.4423e-02, -1.1235e-01, -1.4244e-01, -5.1419e-02, -9.8858e-02,\n",
      "          1.1005e-01,  4.6433e-02, -1.1020e-01, -5.4871e-02, -6.3987e-02,\n",
      "          3.3228e-02, -1.2480e-02,  3.3366e-02,  1.6406e-01, -1.3906e-02,\n",
      "          1.3989e-01, -6.5551e-02,  3.5427e-02,  2.4321e-02, -6.0002e-02,\n",
      "         -2.6559e-02, -1.3691e-01,  8.2562e-02,  1.2485e-01,  2.2141e-01,\n",
      "          1.3228e-01,  1.4269e-01, -1.4917e-01, -4.4262e-02, -8.6120e-02,\n",
      "         -3.5680e-02, -4.1828e-02, -6.4786e-02, -2.1525e-02,  7.4067e-02,\n",
      "          1.0157e-01, -9.6766e-02,  1.2167e-01,  7.6522e-02, -1.5592e-01,\n",
      "          3.0623e-02,  3.2026e-02,  1.1193e-01,  1.3297e-01, -4.9909e-02,\n",
      "          1.7135e-01,  2.5099e-02,  7.8988e-02]]), tensor([2, 4, 1])]\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 2.0831, Val Loss: 2.0774, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 002, Train Loss: 2.0825, Val Loss: 2.0787, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 003, Train Loss: 2.0822, Val Loss: 2.0788, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 004, Train Loss: 2.0786, Val Loss: 2.0792, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 005, Train Loss: 2.0803, Val Loss: 2.0792, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 006, Train Loss: 2.0824, Val Loss: 2.0795, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 007, Train Loss: 2.0800, Val Loss: 2.0787, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 008, Train Loss: 2.0764, Val Loss: 2.0773, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 009, Train Loss: 2.0839, Val Loss: 2.0772, Train Acc: 18%, Val Acc: 10%\n",
      "Epoch: 010, Train Loss: 2.0795, Val Loss: 2.0776, Train Acc: 10%, Val Acc: 19%\n",
      "Epoch: 011, Train Loss: 2.0827, Val Loss: 2.0784, Train Acc: 20%, Val Acc: 21%\n",
      "Epoch: 012, Train Loss: 2.0779, Val Loss: 2.0795, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 013, Train Loss: 2.0803, Val Loss: 2.0791, Train Acc: 17%, Val Acc: 16%\n",
      "Epoch: 014, Train Loss: 2.0783, Val Loss: 2.0805, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 015, Train Loss: 2.0771, Val Loss: 2.0818, Train Acc: 12%, Val Acc: 10%\n",
      "Epoch: 016, Train Loss: 2.0804, Val Loss: 2.0803, Train Acc: 11%, Val Acc: 16%\n",
      "Epoch: 017, Train Loss: 2.0811, Val Loss: 2.0808, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 018, Train Loss: 2.0787, Val Loss: 2.0816, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 019, Train Loss: 2.0765, Val Loss: 2.0826, Train Acc: 17%, Val Acc: 16%\n",
      "Epoch: 020, Train Loss: 2.0812, Val Loss: 2.0828, Train Acc: 16%, Val Acc: 19%\n",
      "Epoch: 021, Train Loss: 2.0757, Val Loss: 2.0827, Train Acc: 17%, Val Acc: 19%\n",
      "Epoch: 022, Train Loss: 2.0776, Val Loss: 2.0813, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 023, Train Loss: 2.0831, Val Loss: 2.0800, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 024, Train Loss: 2.0833, Val Loss: 2.0802, Train Acc: 21%, Val Acc: 16%\n",
      "Epoch: 025, Train Loss: 2.0799, Val Loss: 2.0787, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 026, Train Loss: 2.0813, Val Loss: 2.0790, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 027, Train Loss: 2.0800, Val Loss: 2.0777, Train Acc: 20%, Val Acc: 21%\n",
      "Epoch: 028, Train Loss: 2.0749, Val Loss: 2.0772, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 029, Train Loss: 2.0837, Val Loss: 2.0764, Train Acc: 14%, Val Acc: 16%\n",
      "Epoch: 030, Train Loss: 2.0788, Val Loss: 2.0770, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 031, Train Loss: 2.0802, Val Loss: 2.0775, Train Acc: 18%, Val Acc: 21%\n",
      "Epoch: 032, Train Loss: 2.0822, Val Loss: 2.0779, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 033, Train Loss: 2.0763, Val Loss: 2.0779, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 034, Train Loss: 2.0758, Val Loss: 2.0777, Train Acc: 17%, Val Acc: 16%\n",
      "Epoch: 035, Train Loss: 2.0808, Val Loss: 2.0776, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 036, Train Loss: 2.0749, Val Loss: 2.0783, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 037, Train Loss: 2.0744, Val Loss: 2.0777, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 038, Train Loss: 2.0807, Val Loss: 2.0775, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 039, Train Loss: 2.0758, Val Loss: 2.0780, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 040, Train Loss: 2.0802, Val Loss: 2.0783, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 041, Train Loss: 2.0764, Val Loss: 2.0785, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 042, Train Loss: 2.0830, Val Loss: 2.0786, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 043, Train Loss: 2.0798, Val Loss: 2.0768, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 044, Train Loss: 2.0797, Val Loss: 2.0763, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 045, Train Loss: 2.0813, Val Loss: 2.0764, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 046, Train Loss: 2.0762, Val Loss: 2.0773, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 047, Train Loss: 2.0807, Val Loss: 2.0782, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 048, Train Loss: 2.0838, Val Loss: 2.0785, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 049, Train Loss: 2.0838, Val Loss: 2.0786, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 050, Train Loss: 2.0808, Val Loss: 2.0788, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 051, Train Loss: 2.0749, Val Loss: 2.0784, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 052, Train Loss: 2.0820, Val Loss: 2.0781, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 053, Train Loss: 2.0785, Val Loss: 2.0782, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 054, Train Loss: 2.0791, Val Loss: 2.0787, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 055, Train Loss: 2.0797, Val Loss: 2.0795, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 056, Train Loss: 2.0808, Val Loss: 2.0788, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 057, Train Loss: 2.0774, Val Loss: 2.0789, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 058, Train Loss: 2.0778, Val Loss: 2.0791, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 059, Train Loss: 2.0814, Val Loss: 2.0791, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 060, Train Loss: 2.0779, Val Loss: 2.0784, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 061, Train Loss: 2.0781, Val Loss: 2.0784, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 062, Train Loss: 2.0804, Val Loss: 2.0787, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 063, Train Loss: 2.0774, Val Loss: 2.0788, Train Acc: 14%, Val Acc: 17%\n",
      "Epoch: 064, Train Loss: 2.0771, Val Loss: 2.0790, Train Acc: 18%, Val Acc: 21%\n",
      "Epoch: 065, Train Loss: 2.0776, Val Loss: 2.0792, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 066, Train Loss: 2.0766, Val Loss: 2.0785, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 067, Train Loss: 2.0755, Val Loss: 2.0775, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 068, Train Loss: 2.0778, Val Loss: 2.0769, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 069, Train Loss: 2.0749, Val Loss: 2.0770, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 070, Train Loss: 2.0769, Val Loss: 2.0761, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 071, Train Loss: 2.0816, Val Loss: 2.0752, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 072, Train Loss: 2.0755, Val Loss: 2.0758, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 073, Train Loss: 2.0838, Val Loss: 2.0755, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 074, Train Loss: 2.0745, Val Loss: 2.0764, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 075, Train Loss: 2.0819, Val Loss: 2.0761, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 076, Train Loss: 2.0806, Val Loss: 2.0768, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 077, Train Loss: 2.0737, Val Loss: 2.0768, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 078, Train Loss: 2.0750, Val Loss: 2.0765, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 079, Train Loss: 2.0763, Val Loss: 2.0765, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 080, Train Loss: 2.0763, Val Loss: 2.0766, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 081, Train Loss: 2.0744, Val Loss: 2.0766, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 082, Train Loss: 2.0813, Val Loss: 2.0759, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 083, Train Loss: 2.0763, Val Loss: 2.0750, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 084, Train Loss: 2.0794, Val Loss: 2.0749, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 085, Train Loss: 2.0791, Val Loss: 2.0755, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 086, Train Loss: 2.0840, Val Loss: 2.0762, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 087, Train Loss: 2.0760, Val Loss: 2.0771, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 088, Train Loss: 2.0815, Val Loss: 2.0767, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 089, Train Loss: 2.0768, Val Loss: 2.0755, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 090, Train Loss: 2.0788, Val Loss: 2.0756, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 091, Train Loss: 2.0830, Val Loss: 2.0762, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 092, Train Loss: 2.0789, Val Loss: 2.0763, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 093, Train Loss: 2.0777, Val Loss: 2.0766, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 094, Train Loss: 2.0761, Val Loss: 2.0762, Train Acc: 21%, Val Acc: 19%\n",
      "Epoch: 095, Train Loss: 2.0810, Val Loss: 2.0764, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 096, Train Loss: 2.0726, Val Loss: 2.0771, Train Acc: 19%, Val Acc: 21%\n",
      "Epoch: 097, Train Loss: 2.0768, Val Loss: 2.0776, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 098, Train Loss: 2.0756, Val Loss: 2.0772, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 099, Train Loss: 2.0772, Val Loss: 2.0769, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 100, Train Loss: 2.0732, Val Loss: 2.0763, Train Acc: 22%, Val Acc: 19%\n",
      "Epoch: 101, Train Loss: 2.0782, Val Loss: 2.0757, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 102, Train Loss: 2.0816, Val Loss: 2.0752, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 103, Train Loss: 2.0764, Val Loss: 2.0739, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 104, Train Loss: 2.0807, Val Loss: 2.0749, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 105, Train Loss: 2.0751, Val Loss: 2.0756, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 106, Train Loss: 2.0783, Val Loss: 2.0758, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 107, Train Loss: 2.0848, Val Loss: 2.0763, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 108, Train Loss: 2.0775, Val Loss: 2.0760, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 109, Train Loss: 2.0751, Val Loss: 2.0763, Train Acc: 19%, Val Acc: 21%\n",
      "Epoch: 110, Train Loss: 2.0839, Val Loss: 2.0768, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 111, Train Loss: 2.0756, Val Loss: 2.0773, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 112, Train Loss: 2.0826, Val Loss: 2.0771, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 113, Train Loss: 2.0801, Val Loss: 2.0763, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 114, Train Loss: 2.0741, Val Loss: 2.0757, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 115, Train Loss: 2.0734, Val Loss: 2.0756, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 116, Train Loss: 2.0767, Val Loss: 2.0752, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 117, Train Loss: 2.0789, Val Loss: 2.0752, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 118, Train Loss: 2.0747, Val Loss: 2.0748, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 119, Train Loss: 2.0775, Val Loss: 2.0739, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 120, Train Loss: 2.0772, Val Loss: 2.0736, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 121, Train Loss: 2.0801, Val Loss: 2.0732, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 122, Train Loss: 2.0759, Val Loss: 2.0726, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 123, Train Loss: 2.0755, Val Loss: 2.0724, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 124, Train Loss: 2.0804, Val Loss: 2.0719, Train Acc: 20%, Val Acc: 19%\n",
      "Epoch: 125, Train Loss: 2.0872, Val Loss: 2.0712, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 126, Train Loss: 2.0760, Val Loss: 2.0723, Train Acc: 19%, Val Acc: 21%\n",
      "Epoch: 127, Train Loss: 2.0820, Val Loss: 2.0734, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 128, Train Loss: 2.0757, Val Loss: 2.0742, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 129, Train Loss: 2.0807, Val Loss: 2.0742, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 130, Train Loss: 2.0797, Val Loss: 2.0746, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 131, Train Loss: 2.0744, Val Loss: 2.0741, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 132, Train Loss: 2.0793, Val Loss: 2.0739, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 133, Train Loss: 2.0848, Val Loss: 2.0734, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 134, Train Loss: 2.0773, Val Loss: 2.0736, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 135, Train Loss: 2.0796, Val Loss: 2.0731, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 136, Train Loss: 2.0750, Val Loss: 2.0723, Train Acc: 21%, Val Acc: 16%\n",
      "Epoch: 137, Train Loss: 2.0780, Val Loss: 2.0719, Train Acc: 16%, Val Acc: 16%\n",
      "Epoch: 138, Train Loss: 2.0768, Val Loss: 2.0722, Train Acc: 17%, Val Acc: 21%\n",
      "Epoch: 139, Train Loss: 2.0815, Val Loss: 2.0723, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 140, Train Loss: 2.0808, Val Loss: 2.0728, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 141, Train Loss: 2.0779, Val Loss: 2.0732, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 142, Train Loss: 2.0774, Val Loss: 2.0734, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 143, Train Loss: 2.0739, Val Loss: 2.0732, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 144, Train Loss: 2.0784, Val Loss: 2.0735, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 145, Train Loss: 2.0849, Val Loss: 2.0743, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 146, Train Loss: 2.0769, Val Loss: 2.0753, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 147, Train Loss: 2.0800, Val Loss: 2.0755, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 148, Train Loss: 2.0775, Val Loss: 2.0757, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 149, Train Loss: 2.0826, Val Loss: 2.0764, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 150, Train Loss: 2.0744, Val Loss: 2.0767, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 151, Train Loss: 2.0761, Val Loss: 2.0766, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 152, Train Loss: 2.0758, Val Loss: 2.0765, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 153, Train Loss: 2.0776, Val Loss: 2.0762, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 154, Train Loss: 2.0839, Val Loss: 2.0762, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 155, Train Loss: 2.0766, Val Loss: 2.0752, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 156, Train Loss: 2.0780, Val Loss: 2.0753, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 157, Train Loss: 2.0744, Val Loss: 2.0753, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 158, Train Loss: 2.0804, Val Loss: 2.0749, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 159, Train Loss: 2.0759, Val Loss: 2.0740, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 160, Train Loss: 2.0753, Val Loss: 2.0744, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 161, Train Loss: 2.0812, Val Loss: 2.0745, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 162, Train Loss: 2.0800, Val Loss: 2.0745, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 163, Train Loss: 2.0757, Val Loss: 2.0743, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 164, Train Loss: 2.0782, Val Loss: 2.0732, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 165, Train Loss: 2.0754, Val Loss: 2.0725, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 166, Train Loss: 2.0763, Val Loss: 2.0727, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 167, Train Loss: 2.0778, Val Loss: 2.0727, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 168, Train Loss: 2.0801, Val Loss: 2.0727, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 169, Train Loss: 2.0824, Val Loss: 2.0711, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 170, Train Loss: 2.0772, Val Loss: 2.0712, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 171, Train Loss: 2.0753, Val Loss: 2.0718, Train Acc: 22%, Val Acc: 17%\n",
      "Epoch: 172, Train Loss: 2.0857, Val Loss: 2.0717, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 173, Train Loss: 2.0748, Val Loss: 2.0717, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 174, Train Loss: 2.0796, Val Loss: 2.0716, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 175, Train Loss: 2.0760, Val Loss: 2.0723, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 176, Train Loss: 2.0796, Val Loss: 2.0720, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 177, Train Loss: 2.0821, Val Loss: 2.0723, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 178, Train Loss: 2.0862, Val Loss: 2.0731, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 179, Train Loss: 2.0841, Val Loss: 2.0742, Train Acc: 23%, Val Acc: 21%\n",
      "Epoch: 180, Train Loss: 2.0799, Val Loss: 2.0751, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 181, Train Loss: 2.0760, Val Loss: 2.0753, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 182, Train Loss: 2.0752, Val Loss: 2.0755, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 183, Train Loss: 2.0745, Val Loss: 2.0754, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 184, Train Loss: 2.0803, Val Loss: 2.0759, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 185, Train Loss: 2.0803, Val Loss: 2.0761, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 186, Train Loss: 2.0786, Val Loss: 2.0752, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 187, Train Loss: 2.0819, Val Loss: 2.0749, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 188, Train Loss: 2.0768, Val Loss: 2.0750, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 189, Train Loss: 2.0759, Val Loss: 2.0747, Train Acc: 23%, Val Acc: 19%\n",
      "Epoch: 190, Train Loss: 2.0805, Val Loss: 2.0745, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 191, Train Loss: 2.0775, Val Loss: 2.0742, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 192, Train Loss: 2.0777, Val Loss: 2.0744, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 193, Train Loss: 2.0841, Val Loss: 2.0744, Train Acc: 19%, Val Acc: 19%\n",
      "Epoch: 194, Train Loss: 2.0767, Val Loss: 2.0751, Train Acc: 21%, Val Acc: 21%\n",
      "Epoch: 195, Train Loss: 2.0765, Val Loss: 2.0756, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 196, Train Loss: 2.0810, Val Loss: 2.0759, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 197, Train Loss: 2.0831, Val Loss: 2.0762, Train Acc: 22%, Val Acc: 21%\n",
      "Epoch: 198, Train Loss: 2.0765, Val Loss: 2.0771, Train Acc: 22%, Val Acc: 21%\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_dim = train_data_embeddings.shape[-1]\n",
    "output_dim = len(np.unique([data.y.item() for data in train_data]))\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "# Convert data lists to DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "# train_loader = DataLoader(train_data_embeddings, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_data_embeddings, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_data_embeddings, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the training and evaluation functions\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += pred.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += pred.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "            val_loss += criterion(out, y).item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    return correct / total, val_loss / len(loader), all_labels, all_preds\n",
    "\n",
    "\n",
    "# Get number of classes\n",
    "output_dim = len(np.unique([data.y.item() for data in train_data]))\n",
    "\n",
    "# Initialize model, optimizer, and criterion\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = SimpleGCN(input_dim=3, hidden_dim=64, output_dim=output_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Calculate class weights\n",
    "label_counts = np.bincount([data.y.item() for data in train_data])\n",
    "class_weights = 1.0 / label_counts\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=20, delta=0.001)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 501):\n",
    "    train_loss, train_acc = train()\n",
    "    val_acc, val_loss, _, _ = evaluate(val_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "          f'Train Acc: {int(100 * train_acc):02d}%, Val Acc: {int(100 * val_acc):02d}%')\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
