{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43af987ca3f0d0a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### In this notebook, we will:\n",
    "- Load the CK+ dataset\n",
    "- Save the standard mesh structure\n",
    "- Load the standard mesh graph structure\n",
    "- Split the data into train, validation and test sets\n",
    "- Verify that the label distribution in the original data is maintained in the splits\n",
    "- Visualize the 3D face mesh\n",
    "- Save the data splits to the disk\n",
    "- Load the data splits from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb953e56bc5982",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T18:22:10.783486Z",
     "start_time": "2024-05-30T18:22:06.086999Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447304f98bbf6b4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T18:22:10.971587Z",
     "start_time": "2024-05-30T18:22:10.785059Z"
    }
   },
   "outputs": [],
   "source": [
    "# load ck_landmarks from pickle file\n",
    "ck_landmarks_path = 'ck_data/ck_landmarks.pkl'\n",
    "with open(ck_landmarks_path, 'rb') as f:\n",
    "    ck_landmarks_df = pickle.load(f)\n",
    "num_landmarks = len(ck_landmarks_df['landmarks'][0])\n",
    "print(f\"Number of landmarks: {num_landmarks}, Number of samples: {len(ck_landmarks_df.index)}\")\n",
    "# delete first 500 rows of the dataframe and reset index\n",
    "ck_landmarks_df = ck_landmarks_df.iloc[500:]\n",
    "ck_landmarks_df = ck_landmarks_df.reset_index(drop=True)\n",
    "ck_landmarks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bb7c6cfb20a0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create and save the standard mesh from the mediapipe library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c744afe4efb656d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:29:57.251156Z",
     "start_time": "2024-05-30T16:29:43.223270Z"
    }
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "tesselation = mp.solutions.face_mesh.FACEMESH_TESSELATION\n",
    "G = nx.Graph()\n",
    "for idx, (x, y, z) in enumerate(ck_landmarks_df['landmarks'][0]):\n",
    "    G.add_node(idx, pos=(x, y, z))\n",
    "for connection in tesselation:\n",
    "    G.add_edge(connection[0], connection[1])\n",
    "\n",
    "# Export adjacency matrix\n",
    "np.savetxt(\"standard_mesh_adj_matrix.csv\", nx.to_numpy_array(G), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f3e6380845171",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the graph structure of the standard mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e2b61fd06ac79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T16:30:09.042206Z",
     "start_time": "2024-05-30T16:30:08.995458Z"
    }
   },
   "outputs": [],
   "source": [
    "adjacency_matrix = np.loadtxt('standard_mesh_adj_matrix.csv', delimiter=',')\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "\n",
    "sparsity = 1.0 - np.count_nonzero(adjacency_matrix) / adjacency_matrix.size\n",
    "print(f\"Sparsity of the adjacency matrix: {100*sparsity:.2f}%\")\n",
    "if nx.is_connected(G):\n",
    "    print(f\"Graph is connected\")\n",
    "else:\n",
    "    print(\"Unconnected nodes:\", [node for node, degree in dict(G.degree()).items() if degree == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152013935113e1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Split the data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdca4338ffb41f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:32:16.592255Z",
     "start_time": "2024-05-30T15:32:13.413173Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map string labels to integers\n",
    "label_mapping = {\n",
    "    'neutral': 0,\n",
    "    'happiness': 1,\n",
    "    'sadness': 2,\n",
    "    'surprise': 3,\n",
    "    'fear': 4,\n",
    "    'disgust': 5,\n",
    "    'anger': 6,\n",
    "    'contempt': 7\n",
    "}\n",
    "\n",
    "# Inverse mapping for label names\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Function to convert df to data list using a given graph\n",
    "def df_to_data_list(df, graph):\n",
    "    data_list = []\n",
    "    edge_index = []\n",
    "    \n",
    "    for edge in graph.edges:\n",
    "        edge_index.append([edge[0], edge[1]])\n",
    "        edge_index.append([edge[1], edge[0]])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.int16).t().contiguous()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        landmarks = torch.tensor(row['landmarks'], dtype=torch.float16)\n",
    "        bbox = torch.tensor(row['bbox'], dtype=torch.float16)\n",
    "        label = label_mapping[row['label']]\n",
    "        \n",
    "        data = Data(x=landmarks, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long), bbox=bbox)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "# Function to split data while maintaining label ratio\n",
    "def split_data(df, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-10, \"Ratios must sum to 1\"\n",
    "    \n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    # Group by label\n",
    "    grouped = df.groupby('label')\n",
    "    \n",
    "    for label, group in grouped:\n",
    "        train, temp = train_test_split(group, train_size=train_ratio, stratify=group['label'])\n",
    "        val, test = train_test_split(temp, test_size=test_ratio/(test_ratio + val_ratio), stratify=temp['label'])\n",
    "        \n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "        test_list.append(test)\n",
    "    \n",
    "    # Combine all the splits\n",
    "    train_df = pd.concat(train_list)\n",
    "    val_df = pd.concat(val_list)\n",
    "    test_df = pd.concat(test_list)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Perform the split\n",
    "train_df, val_df, test_df = split_data(ck_landmarks_df)\n",
    "\n",
    "# Convert splits to Data lists using the graph\n",
    "train_data = df_to_data_list(train_df, G)\n",
    "val_data = df_to_data_list(val_df, G)\n",
    "test_data = df_to_data_list(test_df, G)\n",
    "\n",
    "\n",
    "# Print size of each split\n",
    "print(f\"Train: {len(train_data)}  Validation: {len(val_data)}  Test: {len(test_data)}\")\n",
    "\n",
    "# Print a sample data point\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5c999bbeb8c85",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Verify that the label distribution in the original data is maintained in the splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d457b2ccdece727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:32:19.954973Z",
     "start_time": "2024-05-30T15:32:18.664204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inverse mapping for label names\n",
    "inverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "def calculate_label_distribution(data_list):\n",
    "    label_counts = {}\n",
    "    for data in data_list:\n",
    "        label = data.y.item()  # assuming y is a tensor with a single item\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "    \n",
    "    total = sum(label_counts.values())\n",
    "    label_distribution = {label: (count / total) * 100 for label, count in label_counts.items()}\n",
    "    return label_distribution\n",
    "\n",
    "# Calculate distributions\n",
    "original_distribution = calculate_label_distribution(df_to_data_list(ck_landmarks_df, G))\n",
    "train_distribution = calculate_label_distribution(train_data)\n",
    "val_distribution = calculate_label_distribution(val_data)\n",
    "test_distribution = calculate_label_distribution(test_data)\n",
    "\n",
    "def print_ratio_differences(original, train, val, test):\n",
    "    labels = sorted(original.keys())\n",
    "    print(\"\\nLabel Ratios (in percentages):\")\n",
    "    print(\"Label\\t\\tOriginal\\tTrain\\tValidation\\tTest\")\n",
    "    for label in labels:\n",
    "        label_name = inverse_label_mapping[label]\n",
    "        orig_ratio = original.get(label, 0)\n",
    "        train_ratio = train.get(label, 0)\n",
    "        val_ratio = val.get(label, 0)\n",
    "        test_ratio = test.get(label, 0)\n",
    "        print(f\"{label_name.ljust(10)}\\t{orig_ratio:.0f}%\\t\\t\\t{train_ratio:.0f}%\\t\\t{val_ratio:.0f}%\\t\\t\\t{test_ratio:.0f}%\")\n",
    "\n",
    "print_ratio_differences(original_distribution, train_distribution, val_distribution, test_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39ad2c07c0bf7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualize the 3D face mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e0d37ba7e10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:32:21.957873Z",
     "start_time": "2024-05-30T15:32:21.830933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add positions to the graph\n",
    "for i, (x, y, z) in enumerate(ck_landmarks_df['landmarks'][0]):\n",
    "    G.nodes[i]['pos'] = (x, y, z)\n",
    "\n",
    "# Extract node positions\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "# Prepare data for 3D plot\n",
    "edge_x, edge_y, edge_z = [], [], []\n",
    "for edge in G.edges():\n",
    "    x0, y0, z0 = pos[edge[0]]\n",
    "    x1, y1, z1 = pos[edge[1]]\n",
    "    edge_x += [x0, x1, None]\n",
    "    edge_y += [y0, y1, None]\n",
    "    edge_z += [z0, z1, None]\n",
    "\n",
    "node_x, node_y, node_z = [], [], []\n",
    "for node in G.nodes():\n",
    "    x, y, z = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_z.append(z)\n",
    "\n",
    "# Create plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add edges to the plot\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=edge_x, y=edge_y, z=edge_z,\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=2),\n",
    "    hoverinfo='none'\n",
    "))\n",
    "\n",
    "# Add nodes to the plot\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=node_x, y=node_y, z=node_z,\n",
    "    mode='markers',\n",
    "    marker=dict(size=4, color='red'),\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"3D Face Mesh\",\n",
    "    showlegend=False,\n",
    "    scene=dict(\n",
    "        xaxis=dict(showbackground=False),\n",
    "        yaxis=dict(showbackground=False),\n",
    "        zaxis=dict(showbackground=False)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54175ead99f5231",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save and load data splits to / from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c82287605317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:32:24.627250Z",
     "start_time": "2024-05-30T15:32:24.287569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save data splits to disk\n",
    "import pickle\n",
    "\n",
    "# Paths to save the data\n",
    "train_data_path = 'ck_data/train_data_70_20_10.pkl'\n",
    "val_data_path = 'ck_data/val_data_70_20_10.pkl'\n",
    "test_data_path = 'ck_data/test_data_70_20_10.pkl'\n",
    "\n",
    "# Save the data splits\n",
    "with open(train_data_path, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open(val_data_path, 'wb') as f:\n",
    "    pickle.dump(val_data, f)\n",
    "with open(test_data_path, 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "print(\"Data splits saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02484d6a96f77e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:32:26.256660Z",
     "start_time": "2024-05-30T15:32:25.911375Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data splits from disk\n",
    "import pickle\n",
    "\n",
    "# Paths to load the data\n",
    "train_data_path = 'ck_data/train_data_70_20_10.pkl'\n",
    "val_data_path = 'ck_data/val_data_70_20_10.pkl'\n",
    "test_data_path = 'ck_data/test_data_70_20_10.pkl'\n",
    "\n",
    "# Load the data splits\n",
    "with open(train_data_path, 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(val_data_path, 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "with open(test_data_path, 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "print(\"Data splits loaded from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a6c1913bb7127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T18:24:14.706282Z",
     "start_time": "2024-05-30T18:24:13.064050Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "# Function to plot landmarks of the same expression on top of each other\n",
    "def plot_expression_landmarks(df, expression_label):\n",
    "    # Filter the DataFrame by the specified expression label\n",
    "    expression_df = df[df['label'] == expression_label]\n",
    "    \n",
    "    # Create an empty figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add all landmarks of the same expression to the figure\n",
    "    for idx, row in expression_df.iterrows():\n",
    "        landmarks = row['landmarks']\n",
    "        x_vals = [coord[0] for coord in landmarks]\n",
    "        y_vals = [coord[1] for coord in landmarks]\n",
    "        z_vals = [coord[2] for coord in landmarks]\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=x_vals, y=y_vals, z=z_vals,\n",
    "            mode='markers',\n",
    "            marker=dict(size=2, opacity=0.5)\n",
    "        ))\n",
    "    \n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=f'3D Landmarks for Expression: {expression_label}',\n",
    "        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z')\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show(renderer='browser')\n",
    "\n",
    "# List of unique expressions in the DataFrame\n",
    "expressions = ck_landmarks_df['label'].unique()\n",
    "print(expressions)\n",
    "# Plot landmarks for each expression\n",
    "# for expression in expressions:\n",
    "plot_expression_landmarks(ck_landmarks_df, 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee339434caad75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
